{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Jul 13 17:22:02 2024       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX A6000               On  |   00000000:34:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             16W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   1  NVIDIA RTX A6000               On  |   00000000:35:00.0 Off |                  Off |\n",
      "| 30%   32C    P8             20W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   2  NVIDIA RTX A6000               On  |   00000000:36:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             28W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "|   3  NVIDIA RTX A6000               On  |   00000000:37:00.0 Off |                  Off |\n",
      "| 30%   31C    P8             18W /  300W |       2MiB /  49140MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "os.chdir('/home2/kkms4641/GenerativeModel_Tobigs_Conference_20-21/')\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"노을과 함께 해변에 있는 강아지 프로필 사진 만들어줘\" #ex. 우리 강아지한테 한복을 입혀 줘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9347c0c6ca0245638d7882d02b4c7d1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 입력 프롬프트 :  Create a vibrant AI pet profile with a picture of my dog, showcasing a lively and adventurous dog surrounded by a serene beach scene at sunset. The profile should feature a sweet and affectionate dog in the foreground, with a soft and fluffy coat, and a calm and peaceful dog-themed landscape in the background\n"
     ]
    }
   ],
   "source": [
    "# 원하는 경로에 다운 받았을 경우\n",
    "model_id = \"./Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# 별도의 경로 지정 하지 않았을 경우 \n",
    "# model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipeline = transformers.pipeline(\n",
    "  \"text-generation\",\n",
    "  model=model_id,\n",
    "  model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "  device=\"cuda\",\n",
    ")\n",
    "\n",
    "reasoning_prompt = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \n",
    "     \"\"\"You are a prompt engineer who makes prompts for generative AI models. \"\"\"},\n",
    "    \n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \n",
    "     f\"\"\"### instruction ###\n",
    "      I want to make an AI pet profile with a picture of my dog. \n",
    "      The given Input Query Text is the form of the AI pet profile that the user wants. \n",
    "      So what do you think are the conditions for a good Prompt to go through a generative AI model and create a good result image?\n",
    "      and If Input Query is Korean language, Please translate Korean to English.\n",
    "      Please make sure the prompt includes the word 'dog' exactly four time. not others.\n",
    "      The word 'dog' should not appear consecutively.\n",
    "      Finally, Please ensure the generated prompt does not exceed the maximum token length allowed by the CLIP tokenizer.\n",
    "      The maximum token length for the CLIP tokenizer is 60 tokens.\n",
    "     \"\"\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "pipeline.model = pipeline.model.half().to(torch.float32)\n",
    "\n",
    "reasoning_outputs = pipeline(\n",
    "                    reasoning_prompt,\n",
    "                    eos_token_id=terminators,\n",
    "                    do_sample=True,\n",
    "                    temperature=0.6,\n",
    "                    top_p=0.9,\n",
    "                )\n",
    "\n",
    "conditions = reasoning_outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "prompt = [\n",
    "    {\"role\": \"system\", \n",
    "     \"content\": \n",
    "     \"\"\"You are a prompt engineer who makes prompts for generative AI models. \n",
    "        Your answer should always be prompt except explanation.\n",
    "        All prompt factors should be separated by comma(like a, b, c).\"\"\"},\n",
    "    \n",
    "    {\"role\": \"user\", \n",
    "     \"content\": \n",
    "     f\"\"\"### instruction ###\n",
    "    I want to make an AI pet profile with a picture of my dog. \n",
    "      The given Input Query Text is the form of the AI pet profile that the user wants. \n",
    "      So what do you think are the conditions for a good Prompt to go through a generative AI model and create a good result image?\n",
    "      and If Input Query is Korean language, Please translate Korean to English.\n",
    "      Please make sure the output prompt includes the word 'dog' exactly four time. \n",
    "      The word 'dog' should not appear consecutively.\n",
    "      Finally, Please ensure the generated prompt does not exceed the maximum token length allowed by the CLIP tokenizer.\n",
    "      The maximum token length for the CLIP tokenizer is 60 tokens.\n",
    "      ### Input Text ### \n",
    "      Query: {user_query} \n",
    "      Conditions: {conditions}\n",
    "      \n",
    "      =========\n",
    "      New Prompt: \n",
    "     \"\"\"}\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipeline.tokenizer.eos_token_id,\n",
    "    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipeline(\n",
    "    prompt,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    max_new_tokens=60,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "answer = outputs[0][\"generated_text\"][-1]['content']\n",
    "\n",
    "print(\"최종 입력 프롬프트 : \", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create a vibrant AI pet profile with a picture of my dog, showcasing a lively and adventurous dog surrounded by a serene beach scene at sunset. The profile should feature a sweet and affectionate dog in the foreground, with a soft and fluffy coat, and a calm and peaceful dog-themed landscape in the background\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|███████████| 7/7 [00:00<00:00, 1167.87it/s]\n",
      "LoRA : Patching Unet\n",
      "LoRA : Patching text encoder1\n",
      "LoRA : Patching text encoder2\n",
      "LoRA : Patching token input\n",
      "<s1>\n",
      "<s1>_1\n",
      "<s1>_2\n",
      "Create a vibrant AI pet profile with a picture of my <s1> <s1>_1, showcasing a lively and adventurous <s1> <s1>_1 surrounded by a serene beach scene at sunset. The profile should feature a sweet and affectionate <s1> <s1>_1 in the foreground, with a soft and fluffy coat, and a calm and peaceful <s1> <s1>_1-themed landscape in the background\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (90 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (98 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:27<00:00,  1.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.14it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.13it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['<s1>_1 <s1>_2 <s1> <s1>_1 <s1>_2 _ 1 - themed landscape in the background']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['and a calm and peaceful < s 1 > < s 1 >_ 1 - themed landscape in the background']\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:00<00:00,  9.96it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n"
     ]
    }
   ],
   "source": [
    "command_solb = f\"python test_pti_solb_arg.py --prompt \\\"{answer}\\\"\"\n",
    "!{command_solb}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading pipeline components...: 100%|███████████| 7/7 [00:00<00:00, 1175.96it/s]\n",
      "LoRA : Patching Unet\n",
      "LoRA : Patching text encoder1\n",
      "LoRA : Patching text encoder2\n",
      "LoRA : Patching token input\n",
      "<s1>\n",
      "<s1>_1\n",
      "Create a pet profile with a picture of a happy <s1> <s1>_1, featuring a <s1> <s1>_1 wearing a traditional Korean dress, sitting on a green grass, with a cute expression, surrounded by a few <s1> <s1>_1 toys, showcasing a playful and friendly <s1> <s1>_1, with a <s1> <s1>_1's love for its human family, and a loyal <s1> <s1>_1\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (92 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (116 > 77). Running this sequence through the model will result in indexing errors\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: ['love for its human family, and a loyal <s1> <s1>_1 <s1> <s1>_1 _ 1']\n",
      "The following part of your input was truncated because CLIP can only handle sequences up to 77 tokens: [\"< s 1 >_ 1, with a < s 1 > < s 1 >_ 1's love for its human family, and a loyal < s 1 > < s 1 >_ 1\"]\n",
      "100%|███████████████████████████████████████████| 32/32 [00:28<00:00,  1.12it/s]\n",
      "Loading pipeline components...: 100%|█████████████| 5/5 [00:00<00:00,  9.99it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.37it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.36it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.35it/s]\n",
      "100%|█████████████████████████████████████████████| 8/8 [00:05<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "command_jaemin = f\"python test_pti_jaemin_arg.py --prompt \\\"{answer}\\\"\"\n",
    "!{command_jaemin}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
